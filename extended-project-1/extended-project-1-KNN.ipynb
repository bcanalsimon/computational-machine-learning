{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLdXjgq3dNU8"
   },
   "source": [
    "<img src = \"../../Data/bgsedsc_0.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNDWhzFAdNVA"
   },
   "source": [
    "# Extended Project: KNN project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to predict the probability of death of a patient that is entering an ICU (Intensive Care Unit). The dataset comes from MIMIC project (https://mimic.physionet.org/) which refers to the Medical Information Mart for Intensive Care. The preprocessing of the data is the same for both the KNN and SVM projects, with the difference that SVM tackle class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules and auxiliar functions import\n",
    "\n",
    "The relevant modules are loaded and the auxiliar functions from *helper_functions* are imported. It is also set a random seed to ensure reproducibility and it is checked the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m parentdir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(currentdir))\n\u001b[1;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m1\u001b[39m, parentdir)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelper_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(os.path.dirname(currentdir))\n",
    "sys.path.insert(1, parentdir)\n",
    "from utils.helper_functions import *\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import ipywidgets\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import GridSearchCV, HalvingGridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from category_encoders import TargetEncoder\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from scipy.stats import skew\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import AllKNN\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(3123)\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load\n",
    "\n",
    "The **training dataset** is imported as a pandas DataFrame and the first 5 rows are displayed to get an overview of the structure and the contents. The dataset consists of 20885 observations and 44 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjlNkbSSdNVD",
    "outputId": "48511fdd-e873-4e17-8dad-32b2fb66b5f1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('/Users/bertacanal/Desktop/cml23-probability-of-death-with-k-nn/mimic_train.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **test dataset** is imported as a pandas DataFrame and the first 5 rows are displayed to get an overview of the structure and the contents. This dataset will be used to produce predictions and consists of 5221 observations and 39 columns. This dataset does not include the variable *'HOSPITAL_EXPIRE_FLAG'*, therefore, a copy of this dataset is renamed as *X_test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Va4KzjH-dNVI",
    "outputId": "5fc53318-701e-4458-db92-505b22e7382a"
   },
   "outputs": [],
   "source": [
    "data_test=pd.read_csv('/Users/bertacanal/Desktop/cml23-probability-of-death-with-k-nn/mimic_test_death.csv')\n",
    "X_test = data_test.copy()\n",
    "print(X_test.shape)\n",
    "X_test.sort_values('icustay_id').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of the dataset\n",
    "\n",
    "Each observation of the training dataset correspond to the ICU stay of a patient. The outcome to predict is the column HOSPITAL_EXPIRE_FLAG, which indicates if the patient died during the current hospital stay (= 1) or survived (= 0). There are ID columns related to the ICU stay (*hadm_id* and *icustay_id*) and to the patient (*subject_id*). Other columns refer to the vitals of the patient (when entering the ICU) and general information. The specific meaning of each feature is specified in the table below (extracted from the file *mimic_patient_metadata.xlsx*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaning=pd.read_excel('/Users/bertacanal/Desktop/cml23-probability-of-death-with-k-nn/mimic_patient_metadata.xlsx', header = 4)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following bar plot represents the distribution of the *'HOSPITAL_EXPIRE_FLAG'* variable, showing the counts of *Survived* and *Not Survived* categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0: 'Survived', 1: 'Not Survived'}\n",
    "value_counts = data['HOSPITAL_EXPIRE_FLAG'].map(labels).value_counts()\n",
    "\n",
    "total_patients = value_counts.sum()\n",
    "plot = value_counts.plot(kind='bar', rot=0)\n",
    "\n",
    "for index, value in enumerate(value_counts):\n",
    "    percentage = (value / total_patients) * 100\n",
    "    plt.text(index, value, f\"{value} ({percentage:.2f}%)\", ha='center', va='bottom')\n",
    "\n",
    "plt.ylabel('Number of patients')\n",
    "plt.title('Variable HOSPITAL_EXPIRE_FLAG')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addition of metadata\n",
    "\n",
    "As shown in the description of the variables, the *ICD9_diagnosis* column includes the main cause/disease of patient condition. However, a patient can have co-ocurrent diseases (comorbodities). The file containing the secondary codes is *MIMIC_diagnoses.csv* and some merge is required to consider these additional diagnoses. It is assumed that all of the comorbidities were diagnosed on the first day of the patient in the ICU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMIC_diagnoses = pd.read_csv('/Users/bertacanal/Desktop/cml23-probability-of-death-with-k-nn/extra_data/MIMIC_diagnoses.csv')\n",
    "MIMIC_diagnoses.rename(columns={'HADM_ID': 'hadm_id'}, inplace=True)\n",
    "MIMIC_diagnoses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reasonable threshold of diagnoses to consider for the project is set to 20, but given that the training dataset already includes the main one, the merge will only include those ICD9_CODES assigned to SEQ_NUM from 2 to 20. For the missing data it is created a missing class called *MISSING_DIAGNOSIS*. Both dataframes are join on 'hadm_id' (which refers to the hospital stay identifier) because the *MIMIC_diagnoses.csv* includes information about different hospital stays of the different of each patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_num in range (2, 21):\n",
    "    secondary_codes = MIMIC_diagnoses[MIMIC_diagnoses['SEQ_NUM'] == seq_num].copy()\n",
    "    secondary_codes.head()\n",
    "\n",
    "    secondary_codes[f'ICD9_CODE_{seq_num}'] = secondary_codes['ICD9_CODE']\n",
    "    secondary_codes.head()\n",
    "    data = pd.merge(data,secondary_codes[['hadm_id',f'ICD9_CODE_{seq_num}']],on='hadm_id', how='left')\n",
    "    data[f'ICD9_CODE_{seq_num}'].fillna('MISSING_DIAGNOSIS', inplace = True)\n",
    "    X_test = pd.merge(X_test,secondary_codes[['hadm_id',f'ICD9_CODE_{seq_num}']],on='hadm_id', how='left')\n",
    "    X_test[f'ICD9_CODE_{seq_num}'].fillna('MISSING_DIAGNOSIS', inplace = True)\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = []\n",
    "\n",
    "for seq_num in range (2, 21):\n",
    "    column_name = f'ICD9_CODE_{seq_num}'\n",
    "    if column_name in data.columns:\n",
    "        counts = data[column_name].value_counts()\n",
    "        if 'MISSING_DIAGNOSIS' in counts:\n",
    "            proportion = round((counts['MISSING_DIAGNOSIS'] / data.shape[0])*100,2)\n",
    "            proportions.append((column_name, proportion))\n",
    "proportions_df = pd.DataFrame(proportions, columns=['Column Name', 'Proportion of \"MISSING_DIAGNOSIS\" (%)'])\n",
    "proportions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the proportion of missing data for those columns referring to a higher sequence number is high, it seems reasonable to drop some of those columns. Therefore, this project will set as threshold a proportion of 40% of missing data for the secondary codes and thus only consider until the column referring to the sequence number 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['ICD9_CODE_13', 'ICD9_CODE_14', 'ICD9_CODE_15', 'ICD9_CODE_16', 'ICD9_CODE_17', 'ICD9_CODE_18', 'ICD9_CODE_19', 'ICD9_CODE_20'], axis=1)\n",
    "X_test = X_test.drop(['ICD9_CODE_13', 'ICD9_CODE_14', 'ICD9_CODE_15', 'ICD9_CODE_16', 'ICD9_CODE_17', 'ICD9_CODE_18', 'ICD9_CODE_19', 'ICD9_CODE_20'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dicernment between *X_train* and *y_train*\n",
    "\n",
    "From the training dataset it should be distinguished between *X_train* (including all the features) and *y_train* (including the variable *HOSPITAL_EXPIRE_FLAG*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.drop(['HOSPITAL_EXPIRE_FLAG'], axis=1)\n",
    "y_train = data['HOSPITAL_EXPIRE_FLAG']\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code aims to compare the dimensions from *X_train* and *X_test*. As expected, the number of observations of the training dataset is higher than the testing dataset since we want to feed the model with as much data as possible to find and learn meaningful patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [X_train.shape, X_test.shape]\n",
    "dimensions_df = pd.DataFrame([(int(dim[0]), int(dim[1])) for dim in dimensions], index= [\"X_train\", \"X_test\"], columns=[\"Rows\", \"Columns\"])\n",
    "dimensions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know which columns appear in *X_train* but not in *X_test* it is done a set difference operation. It is useful to know the columns in each dataset for the step related to drop the features not used in the model. As seen in the output, the columns that only appear in *X_train* correspond to *'DEATHTIME'*, *'DISCHTIME'*, *'DOD'* and *'LOS'*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_X_train = set(X_train.columns)\n",
    "columns_X_test = set(X_test.columns)\n",
    "\n",
    "columns_only_in_X_train = columns_X_train - columns_X_test\n",
    "columns_only_in_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the *AGE* variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The age could have an impact on the probability of death of a patient that is entering an ICU, however, there is no specific feature in the dataset referring to the age. An alternative could be to subtract the *DOB* (Date of Birth) from the *DOD* (Date of Death) but it has to be disregarded since the *DOD* is not known in the first day of a patient in an ICU and moreover it does not allow to compute the age of the patients who survive.\n",
    "\n",
    "Therefore, the procedure followed consists on subtracting the *DOB* (Date of Birth) from the *ADMITTIME* (Admision datetime) since this results on the age of the patient in the admission day. These two date columns are converted to datatime objects and then the age is computed using the *relativedelta* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['DOB'] = pd.to_datetime(X_train['DOB'], format='%Y-%m-%d %H:%M:%S')\n",
    "X_train['ADMITTIME'] = pd.to_datetime(X_train['ADMITTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "X_train['AGE'] = X_train.apply(lambda row: relativedelta(row['ADMITTIME'], row['DOB']).years, axis=1)\n",
    "\n",
    "X_train['AGE'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['DOB'] = pd.to_datetime(X_test['DOB'], format='%Y-%m-%d %H:%M:%S')\n",
    "X_test['ADMITTIME'] = pd.to_datetime(X_test['ADMITTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "X_test['AGE'] = X_test.apply(lambda row: relativedelta(row['ADMITTIME'], row['DOB']).years, axis=1)\n",
    "\n",
    "X_test['AGE'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjustments to the variable *AGE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quartiles and the mean seem to be reasonable. However, since the maximum value of the column *AGE* corresponds to 310 in both the training and testing data, an adjustment is needed.  The problem is that the age of patients older than 89 has been fixed to 300 at their first admission (Source: https://mimic.mit.edu/docs/iii/about/time/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Patients older than 89:')\n",
    "print(X_train[X_train['AGE'] > 89]['AGE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, it is reasonable to readjust the *AGE* by assigning 89 years as the maximum patient's age:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['AGE'] = X_train['AGE'].apply(lambda age: min(age, 89))\n",
    "X_test['AGE'] = X_test['AGE'].apply(lambda age: min(age, 89))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vital signs are considered important in medical care and disease diagnosis. They include heart rate, blood pressure, respiratory rate and body temperature. For example, a significant increase or decrease in heart rate may be an indicator of heart problems, high blood pressure can increase the risk of cardiovascular diseases, changes in respiratory rate can indicate respiratory or cardiovascular problems and a significant increase or decrease in temperature can be a sign of infection. Therefore, features related to the minimum, maximum and mean of the vital signs are included in the model.\n",
    "\n",
    "Both the main disease and comorbidities play significant roles in determining a patient's probability of death. The main disease's severity directly influences the risk of mortality. Comorbidities, on the other hand, add complexity to the patient's health profile: they can complicate treatment, reduce the body's ability to respond to stress and weaken the immune system. Therefore, features related to the main disease and comorbodities are included in the model.\n",
    "\n",
    "Both gender and age can impact a patient's probability of survival in the ICU. Gender can influence because of differences in biological, hormonal, and treatment responses and age affects physiological reserves and vulnerability to illness.\n",
    "\n",
    "Patients admitted to the ICU in an emergency, such as after a sudden cardiac arrest or trauma, may have a higher initial severity of illness. This can increase the risk of mortality, therefore it seems reasonable that admission type infers on the probability of death. The first care unit to which a patient is assigned can provide valuable insights into the initial assessment of the patient's condition and potential factors that may influence their probability of death. At the same time, the type of insurance also infers because it affects the access to care and the care quality, since patients with certain insurance plans may receive care at facilities with higher ICU quality ratings, potentially leading to better outcomes. Marital status can also be an influenting factor through factors such as social support, advocacy and emotional well-being. For example, married patients may benefit from spousal support or better mental health. \n",
    "\n",
    "In many healthcare systems, medical care and treatment standards are designed to be uniform and evidence-based, focusing on clinical needs rather than ethnic or religious considerations. Therefore, ethnicity and religion are not included in the model. ID variables are also excluded since these are not predictive of death and add noise into the model, thus decreasing its performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features not included in the model are dropped. From the training dataset we exclude the ID variables (*subject_id*, *hadm_id* and *icustay_id*), the features we don't know the first day of a patient in an ICU (*DOD*, *DISCHTIME*, *DEATHTIME* and *LOS*) and the variables we consider not to infer in the probability of death of a patient (*ETHNICITY*, *RELIGION* and *Diff*). In addition, the variables used to create the *AGE* variable, in particular *DOB* and *ADMITTIME*, are also excluded since its information is already contained by the model with the created *AGE* variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['subject_id', 'hadm_id', 'icustay_id', 'DOB', 'DOD', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'Diff', 'LOS', 'ETHNICITY', 'RELIGION'], axis=1)\n",
    "print(X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features not included in the model are dropped. From the testing dataset we exclude the ID variables (*subject_id*, *hadm_id* and *icustay_id*) and the variables we consider not to infer in the probability of death of a patient (*ETHNICITY*, *RELIGION* and *Diff*). In addition, the variables used to create the *AGE* variable, in particular *DOB* and *ADMITTIME*, are also excluded since its information is already contained by the model with the created *AGE* variable.\n",
    "\n",
    "As seen before, *X_test* does not contain the columns *'DEATHTIME'*, *'DISCHTIME'*, *'DOD'* and *'LOS'*. Therefore, these columns should not be droped from the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(['subject_id', 'hadm_id', 'icustay_id', 'DOB','ADMITTIME', 'Diff', 'ETHNICITY', 'RELIGION'], axis=1)\n",
    "print(X_test.shape)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping the variables, it can be observed that the number of columns on both *X_train* and *X_test* is equal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [X_train.shape, X_test.shape]\n",
    "dimensions_df = pd.DataFrame([(int(dim[0]), int(dim[1])) for dim in dimensions], index= [\"X_train\", \"X_test\"], columns=[\"Rows\", \"Columns\"])\n",
    "dimensions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a list of the names of the categorical and numerical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_names = ['GENDER', 'INSURANCE', 'ADMISSION_TYPE', 'MARITAL_STATUS', 'FIRST_CAREUNIT', 'ICD9_diagnosis', 'DIAGNOSIS', 'ICD9_CODE_2', 'ICD9_CODE_3', 'ICD9_CODE_4', 'ICD9_CODE_5', 'ICD9_CODE_6', 'ICD9_CODE_7', 'ICD9_CODE_8', 'ICD9_CODE_9', 'ICD9_CODE_10', 'ICD9_CODE_11', 'ICD9_CODE_12']\n",
    "\n",
    "numerical_features_names = [name for name in X_train.columns if name not in categorical_features_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cardinality of categorical features\n",
    "\n",
    "Categorical features will be treated differently depending on the cardinality. The following output provides information about the categorical features: *count* corresponds to the number of non-null values, *unique* indicates the number of unique categories in the feature, *top* refers to the most frequent category and *freq* corresponds to the frequency in which the top category occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_summary = []\n",
    "\n",
    "for feature in categorical_features_names:\n",
    "    summary = X_train[feature].describe()\n",
    "    categorical_summary.append(summary)\n",
    "\n",
    "categorical_summary_table = pd.DataFrame(categorical_summary, index=categorical_features_names)\n",
    "categorical_summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the output of the previous code, the low cardinality variables consist on *GENDER*, *INSURANCE*, *ADMISSION_TYPE*, *MARITAL_STATUS* and *FIRST_CAREUNIT* while the variables with high cardinality consists on the *DIAGNOSIS* feature and the variables including ICD9 codes.\n",
    "\n",
    "The output is also useful to identify that most of the patients from the training dataset are males, the most frequent marital status is to be married, the insurance is Medicare, the admission type is emergency and the first ICU assigned is Medical Intensive Care Unit (MICU). \n",
    "\n",
    "Regarding the diagnoses, the most frequent main disease corresponds to *Coronary atherosclerosis of native coronary artery*. The most frequent comorbidities are *Acute kidney failure, unspecified* and *Unspecified essential hypertension*. This later information has been extracted from the file *MIMIC_metadata_diagnose* which includes the meaning of the ICD9 codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaning_diagnoses = pd.read_csv('/Users/bertacanal/Desktop/cml23-probability-of-death-with-k-nn/extra_data/MIMIC_metadata_diagnose.csv')\n",
    "target_ICD9_CODES = ['41401', '5849', '4019']\n",
    "meaning_diagnoses[meaning_diagnoses['ICD9_CODE'].isin(target_ICD9_CODES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment of the low cardinality categorical features\n",
    "\n",
    "Focusing on the low cardinality categorical features, let's see more in detail how many observations are included in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train['GENDER'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train['FIRST_CAREUNIT'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train['INSURANCE'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train['ADMISSION_TYPE'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train['MARITAL_STATUS'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that *MARITAL_STATUS* has categories with a low number of observations, it is reasonable to group the categories that includes less than 5% of the total observations in *X_train*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marital_status_counts = X_train['MARITAL_STATUS'].value_counts()\n",
    "categories_to_replace = marital_status_counts[marital_status_counts < 0.05 * len(X_train)].index\n",
    "\n",
    "X_train.loc[X_train['MARITAL_STATUS'].isin(categories_to_replace), 'MARITAL_STATUS'] = 'OTHER'\n",
    "X_test.loc[X_test['MARITAL_STATUS'].isin(categories_to_replace), 'MARITAL_STATUS'] = 'OTHER'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, this preprocessing step group the categories *'SEPARATED'*, *'UNKNOWN (DEFAULT)'* and *'LIFE PARTNER'* with the category *'OTHERS'*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train['MARITAL_STATUS'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the low cardinality categorical features are depicted in the following graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_cardinality_categorical_names = ['GENDER', 'INSURANCE', 'ADMISSION_TYPE', 'MARITAL_STATUS', 'FIRST_CAREUNIT']\n",
    "\n",
    "fcols = 3\n",
    "frows = (len(low_cardinality_categorical_names) + fcols - 1) // fcols\n",
    "plt.figure(figsize=(15, 4 * frows))\n",
    "\n",
    "for i, col in enumerate(low_cardinality_categorical_names):\n",
    "    plt.subplot(frows, fcols, i + 1)\n",
    "    sns.countplot(data=X_train, x=col)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle missing data\n",
    "\n",
    "Missing data will be handled differently depending on the type of feature: low cardinality categorical features, high cardinality categorical features and numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low cardinality categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to check the null values per feature for both the X_train and X_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_low_cardinality = X_train[low_cardinality_categorical_names]\n",
    "X_train_low_cardinality.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_low_cardinality = X_test[low_cardinality_categorical_names]\n",
    "X_test_low_cardinality.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous output indicates that the only low cardinality categorical feature with missing data corresponds to *MARITAL_STATUS*. To handle missing data from this feature, the *SimpleImputer* will be used to replace missing values by the most frequent value of this column. To check that this step has been done correctly, we can see that there is no missing data in the low cardinality categorical features from both datasets after the imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_frequent = SimpleImputer(missing_values = np.nan, strategy= 'most_frequent')\n",
    "\n",
    "imp_frequent.fit(X_train_low_cardinality)\n",
    "\n",
    "X_train_categorical_transform = pd.DataFrame(imp_frequent.transform(X_train_low_cardinality), columns = low_cardinality_categorical_names)\n",
    "X_test_categorical_transform = pd.DataFrame(imp_frequent.transform(X_test_low_cardinality), columns = low_cardinality_categorical_names)\n",
    "\n",
    "X_train[low_cardinality_categorical_names] = X_train_categorical_transform\n",
    "X_test[low_cardinality_categorical_names] = X_test_categorical_transform\n",
    "\n",
    "print(\"Missing data in low cardinality categorical features from X_train: \", X_train[low_cardinality_categorical_names].isnull().sum().sum())\n",
    "print(\"Missing data in low cardinality categorical features from X_test: \", X_test[low_cardinality_categorical_names].isnull().sum().sum())\n",
    "\n",
    "X_train[low_cardinality_categorical_names].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High cardinality categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to check the null values per feature for both the X_train and X_test. Since it was created a missing class for the missing data from the secondary codes, there aren't null values present in the high cardinality categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cardinality_categorical_names = ['ICD9_diagnosis', 'DIAGNOSIS', 'ICD9_CODE_2', 'ICD9_CODE_3', 'ICD9_CODE_4', 'ICD9_CODE_5', 'ICD9_CODE_6', 'ICD9_CODE_7', 'ICD9_CODE_8', 'ICD9_CODE_9', 'ICD9_CODE_10', 'ICD9_CODE_11', 'ICD9_CODE_12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_high_cardinality = X_train[high_cardinality_categorical_names]\n",
    "X_train_high_cardinality.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_high_cardinality = X_test[high_cardinality_categorical_names]\n",
    "X_test_high_cardinality.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous cases, let's first check the missing data per numerical feature in both X_train and X_test. All numerical features has missing data except for the variable *AGE*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numerical_columns = X_train[numerical_features_names]\n",
    "X_train_numerical_columns.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_numerical_columns = X_test[numerical_features_names]\n",
    "X_test_numerical_columns.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to deal with missing data from numerical features will consists on removing columns with a considerable high proportion of missing data. In this case it is established a percentage of 20% as the percentage of non-null answers required to keep a column. As can be seen, the dimensions before and after computing this step are the same, therefore it has not removed any column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_columns = X_train_numerical_columns\n",
    "\n",
    "print(X_train.shape)\n",
    "X_train = X_train.dropna(axis=1, thresh=round(0.2 * len(X_train.index)))\n",
    "print(X_train.shape)\n",
    "\n",
    "dropped_columns = list(set(initial_columns) - set(X_train.columns))\n",
    "X_test = X_test.drop(columns=dropped_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing data from the vitals of the patient (when entering the ICU) can be imputed by the mean. A possible consideration could be to impute group by gender, therefore using the mean of the corresponding gender to impute the vitals. This option was disregarded since the means of the vitals do not vary much between females and males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[[\"HeartRate_Mean\", \"SysBP_Mean\", \"DiasBP_Mean\", \"MeanBP_Mean\", \"RespRate_Mean\", \"TempC_Mean\", \"SpO2_Mean\", \"Glucose_Mean\", \"GENDER\"]].groupby(\"GENDER\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it seems not reasonable to use stratified average statistics to impute the vitals, it will be proceed with the *KNNImputer* to deal with missing data using k-Nearest Neighbors. Therefore, the missing data from the vitals will be imputed using the mean value from n_neighbors nearest neighbors found in the training dataset. The parameter *weights* is set to *'distance'* to weight points by the inverse of their distance and *n_neighbors* is set to 5. To check that this step has been done correctly, we can see that there is no missing data in the numerical features from both datasets after the imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_knn = KNNImputer(n_neighbors= 5, weights = 'distance')\n",
    "\n",
    "imp_knn.fit(X_train[numerical_features_names])\n",
    "\n",
    "X_train[numerical_features_names] = pd.DataFrame(imp_knn.transform(X_train[numerical_features_names]), columns = numerical_features_names)\n",
    "X_test[numerical_features_names] = pd.DataFrame(imp_knn.transform(X_test[numerical_features_names]), columns = numerical_features_names)\n",
    "\n",
    "print(\"Missing data in numerical features from X_train: \", X_train[numerical_features_names].isnull().sum().sum())\n",
    "print(\"Missing data in numerical features from X_test: \", X_test[numerical_features_names].isnull().sum().sum())\n",
    "\n",
    "X_train[numerical_features_names].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skewness is checked for the numerical features. The feature is considered slightly skewed if the skewness is between -1 and -0.5 (negative skewed) or between 0.5 and 1 (positive skewed). The feature is considered extremely skewed if the skewness is lower than -1 (negative skewed) or greater than 1 (positive skewed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness_results = {}\n",
    "num_features = len(numerical_features_names)\n",
    "num_plots_per_row = 4\n",
    "num_rows = (num_features + num_plots_per_row - 1) // num_plots_per_row\n",
    "\n",
    "# Create a figure and an array of subplots\n",
    "fig, axes = plt.subplots(num_rows, num_plots_per_row, figsize=(16, 4 * num_rows))\n",
    "\n",
    "# Loop through each numerical feature\n",
    "for i, col in enumerate(numerical_features_names):\n",
    "    col_name = col\n",
    "    col_skew = skew(X_train[col])\n",
    "    col_skew_rounded = round(col_skew, 4)\n",
    "    skewness_results[col_name] = col_skew_rounded\n",
    "    row_idx = i // num_plots_per_row\n",
    "    col_idx = i % num_plots_per_row\n",
    "    \n",
    "    # Plot a histogram in the current subplot\n",
    "    sns.histplot(X_train[col], kde=True, ax=axes[row_idx, col_idx])\n",
    "    axes[row_idx, col_idx].set_title(f'Skewness for {col_name}: {col_skew_rounded}')\n",
    "    \n",
    "# Remove empty subplots\n",
    "for i in range(num_features, num_rows * num_plots_per_row):\n",
    "    row_idx = i // num_plots_per_row\n",
    "    col_idx = i % num_plots_per_row\n",
    "    fig.delaxes(axes[row_idx, col_idx])\n",
    "\n",
    "# Adjust layout spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show all the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A log transformation is applied to handle extremely right-skewed features. Applying this transformtaion is useful for the skewed data since the distribution resembles more a normal. It is also useful to decrease the effect of outliers, thus obtaining a more robust model. As shown in the output, the log transformed numerical features log transformed correspond to *'DiasBP_Max'*, *'MeanBP_Max'*, *'RespRate_Max'*, *'Glucose_Min'*, *'Glucose_Max'* and *'Glucose_Mean'*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_transform = []\n",
    "\n",
    "for col in numerical_features_names:\n",
    "    col_skew = skew(X_train[col])\n",
    "    if col_skew > 1:\n",
    "        variables_to_transform.append(col)\n",
    "\n",
    "for col in variables_to_transform:\n",
    "    X_train[col] = np.log(X_train[col])\n",
    "    X_test[col] = np.log(X_test[col])\n",
    "\n",
    "print(\"Numerical features log transformed:\", variables_to_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation among numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation among numeric features is computed since highly correlated features do not bring a considerable additional information, thus leading to collinearity issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X_train[numerical_features_names].corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sources set a Pearson's correlation coefficient of 0.80 as the threshold to consider two variables highly correlated. Therefore, the following output aims to find pairs of features with correlations higher than the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.80\n",
    "\n",
    "correlation_pairs = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "            feature1 = correlation_matrix.columns[i]\n",
    "            feature2 = correlation_matrix.columns[j]\n",
    "            correlation = correlation_matrix.iloc[i, j]\n",
    "            correlation_pairs.append((feature1, feature2, correlation))\n",
    "\n",
    "for feature1, feature2, correlation in correlation_pairs:\n",
    "    print(f\"Correlation between {feature1} and {feature2}: {correlation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To avoid collinearity issues, the numerical variables *HeartRate_Mean*, *MeanBP_Mean*, *TempC_Mean* and *Glucose_Mean* are dropped from both the training and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['HeartRate_Mean', 'MeanBP_Mean', 'TempC_Mean', 'Glucose_Mean'], axis = 1)\n",
    "X_test = X_test.drop(['HeartRate_Mean', 'MeanBP_Mean', 'TempC_Mean', 'Glucose_Mean'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low cardinality categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low cardinality categorical variables will be encoded using one-hot encoding to transform categorical variables into dummies. The parameter *drop_first = True* is used to get k-1 dummies out of k categorical levels by removing the first level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = ['GENDER', 'INSURANCE', 'ADMISSION_TYPE', 'MARITAL_STATUS', 'FIRST_CAREUNIT']\n",
    "\n",
    "for col in columns_to_encode:\n",
    "    X_train = pd.get_dummies(X_train, prefix=[col], columns=[col], drop_first=True)\n",
    "    X_test = pd.get_dummies(X_test, prefix=[col], columns=[col], drop_first=True)\n",
    "\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High cardinality categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For high cardinality categorical variables target encoder is used with *TargetEncoder* from the *Category Encoders* package. According to the documentation from *TargetEncoder*, for the case of a categorical target (in our case 'HOSPITAL_EXPIRE_FLAG'), features are replaced with a blend of posterior probability of the target given particular categorical value and the prior probability of the target over all the training data (Source: https://contrib.scikit-learn.org/category_encoders/targetencoder.html). *Smoothing* corresponds to the smoothing effect to balance categorical average versus the prior. This parameter is set to 10 because if the value is increased a lot then it contributes to overfitting and if it is more closest to 0 (since it should be strictly larger than 0) then the model looses predictive accuracy. The target encoder is fitted with training data and applied to both training and testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['ICD9_diagnosis', 'DIAGNOSIS', 'ICD9_CODE_2', 'ICD9_CODE_3', 'ICD9_CODE_4', 'ICD9_CODE_5', 'ICD9_CODE_6', 'ICD9_CODE_7', 'ICD9_CODE_8', 'ICD9_CODE_9', 'ICD9_CODE_10', 'ICD9_CODE_11', 'ICD9_CODE_12']\n",
    "\n",
    "for col in columns:\n",
    "    te = TargetEncoder(smoothing = 10)\n",
    "    te.fit(X = X_train[col], y = y_train)\n",
    "    values_train = te.transform(X_train[col])\n",
    "    values_test = te.transform(X_test[col])\n",
    "    X_train[col] = values_train\n",
    "    X_test[col] = values_test\n",
    "\n",
    "X_train[columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform metadata columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The addition of secondary codes increase the complexity of the model, therefore it seems reasonable to convert those columns into summary statistics and drop the initial columns from metadata. For example, including the mean can be useful to capture the central tendency of comorbidities for each patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert_statistics = ['ICD9_CODE_2', 'ICD9_CODE_3', 'ICD9_CODE_4', 'ICD9_CODE_5', 'ICD9_CODE_6', 'ICD9_CODE_7', 'ICD9_CODE_8', 'ICD9_CODE_9', 'ICD9_CODE_10', 'ICD9_CODE_11', 'ICD9_CODE_12']\n",
    "    \n",
    "X_train['METADATA_MEAN'] = X_train[columns_to_convert_statistics].mean(axis=1)\n",
    "X_train['METADATA_MAX'] = X_train[columns_to_convert_statistics].max(axis=1)\n",
    "\n",
    "X_train = X_train.drop(columns_to_convert_statistics, axis=1)\n",
    "\n",
    "X_test['METADATA_MEAN'] = X_test[columns_to_convert_statistics].mean(axis=1)\n",
    "X_test['METADATA_MAX'] = X_test[columns_to_convert_statistics].max(axis=1)\n",
    "\n",
    "X_test = X_test.drop(columns_to_convert_statistics, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the dimensions from both the training and testing dataset had been reduced by 9 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features are scaled using *StandardScaler*. The parameters *with_mean* and *with_std* are set to *True* which implies that data is centered before scaling and it scale the data to unit variance. The scaler is fitted with the training dataset and then applied to both training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler(with_mean = True, with_std = True)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train) \n",
    "X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it is created an instance of a K-nearest neighbors classifier (KNeighborsClassifier) and set the *algorithm* parameter to *'brute'* which involves the brute-force computation of distances between all pairs of observations in the dataset. Performing grid search over *algorithm* is used to make the algorithm faster not to improve performance, that is why it is not included in the GridSearch.\n",
    "\n",
    "The hyperparameter optimization is done before fitting the model. The parameters tuned in the GridSearch corresponds to:\n",
    "- *n_neighbors* which is the number of neighbors. This parameter is grid search over a range from 50 to 350 with increases of 50 since a low number of neighbors increases variance but a high number increases bias.\n",
    "- *weights* which refers to the weight function used in prediction. While *uniform* distance uses uniform weights, *distance* weights observations by the inverse of the distance.\n",
    "- *metric* which corresponds to the distance measure.\n",
    "\n",
    "Then the SVM model is fitted with the *X_train* and *y_train*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyKNN = KNeighborsClassifier(algorithm =  'brute')\n",
    "\n",
    "grid_values = {'n_neighbors':[100, 150, 200, 250, 300, 350, 400, 450], 'weights':['uniform','distance'], 'metric': ['manhattan', 'minkowski']}\n",
    "\n",
    "grid_knn_acc = GridSearchCV(MyKNN, param_grid = grid_values, scoring = 'roc_auc', n_jobs = -1, cv = 20) # CV defines the number of folds \n",
    "\n",
    "grid_knn_acc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal parameters from the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal parameters from the grid correspond to 350 neighbors, the optimal *weights* is *'distance'* and the *metric* is *'manhattan'*. The resulting best score equals 0.93683. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearch_table_plot(grid_knn_acc, \"n_neighbors\", negative=False, display_all_params=False)\n",
    "\n",
    "print('Best k parameter : '+ str(grid_knn_acc.best_estimator_.n_neighbors))\n",
    "print('Best weights parameter : '+ str(grid_knn_acc.best_estimator_.weights))\n",
    "print('Best metric parameter : '+ str(grid_knn_acc.best_estimator_.metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-sample predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The out-of-sample predictions uses the data from the *X_test* which has not been used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_outsample_pred_prob = grid_knn_acc.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-sample predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The in-sample predictions uses the data from the *X_train* which has been used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_insample_pred_prob = grid_knn_acc.predict_proba(X_train)\n",
    "\n",
    "print(\"AUC: \", roc_auc_score(y_train, knn_insample_pred_prob[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC AUC score obtained from the in-sample prediction corresponds to 1. Since the score is 1, it suggests that the model perfectly distinguish between patients who die or survive. Through the following confusion matrix it can be seen that all the observations from the training dataset are correctly classified, thus obtaining a False Positive Rate and False Negative Rate of 0. Said result can also suggest overfitting, which may be addressed as a future line to improve the project. However, the optimal parameters have been selected using cross-validation, which mitigates the effect of overfitting, that is why the best score equals 0.93683."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix\")\n",
    "cm=confusion_matrix(y_train,knn_insample_pred_prob[:,1])\n",
    "plot_confusion_matrix(cm, ['Survived','Not survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output predictions file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code produce a csv file with the output predictions to submit them to Kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_submit_knn = pd.DataFrame({\"icustay_id\": data_test[\"icustay_id\"], \"HOSPITAL_EXPIRE_FLAG\": knn_outsample_pred_prob[:, 1]})\n",
    "test_predictions_submit_knn.to_csv(\"test_predictions_submit_knn.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file containing the output predictions has the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_submit_knn.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
